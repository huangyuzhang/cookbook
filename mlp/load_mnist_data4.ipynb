{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_mnist import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load traditional MNIST dataset\n",
    "This dataset is taken from http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_tr, labels_tr = load_mnist('MNIST/')\n",
    "images_tst, labels_tst = load_mnist('MNIST/', 't10k')\n",
    "\n",
    "X_tr, y_tr = images_tr, labels_tr\n",
    "X_tst, y_tst = images_tst, labels_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X_tr[:12000:600], X_tr[13000:30600:600], X_tr[30600:60000:590]]\n",
    "plot_images(example_images, images_per_row=10)\n",
    "# save_fig(\"more_images_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images for each label\n",
    "X_0 = X_tr[(y_tr == 0)]\n",
    "X_1 = X_tr[(y_tr == 1)]\n",
    "X_2 = X_tr[(y_tr == 2)]\n",
    "X_3 = X_tr[(y_tr == 3)]\n",
    "X_4 = X_tr[(y_tr == 4)]\n",
    "X_5 = X_tr[(y_tr == 5)]\n",
    "X_6 = X_tr[(y_tr == 6)]\n",
    "X_7 = X_tr[(y_tr == 7)]\n",
    "X_8 = X_tr[(y_tr == 8)]\n",
    "X_9 = X_tr[(y_tr == 9)]\n",
    "\n",
    "plt.figure(figsize=(24,24))\n",
    "plt.subplot(521); plot_images(X_0[:25], images_per_row=5)\n",
    "plt.subplot(522); plot_images(X_1[:25], images_per_row=5)\n",
    "plt.subplot(523); plot_images(X_2[:25], images_per_row=5)\n",
    "plt.subplot(524); plot_images(X_3[:25], images_per_row=5)\n",
    "plt.subplot(525); plot_images(X_4[:25], images_per_row=5)\n",
    "plt.subplot(526); plot_images(X_5[:25], images_per_row=5)\n",
    "plt.subplot(527); plot_images(X_6[:25], images_per_row=5)\n",
    "plt.subplot(528); plot_images(X_7[:25], images_per_row=5)\n",
    "plt.subplot(529); plot_images(X_8[:25], images_per_row=5)\n",
    "plt.subplot(5,2,10); plot_images(X_9[:25], images_per_row=5)\n",
    "# save_fig(\"images_for_each_label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the values of the image data into the interval [0.01, 0.99] by dividing (255 * 0.98 + 0.01), which put values between 0 and 1 but not including 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_tr / 255 * 0.98 + 0.01\n",
    "X_tst = X_tst / 255 * 0.98 + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select 5 & 6 from training and test\n",
    "train_filter = np.where((y_tr == 5 ) | (y_tr == 6))\n",
    "test_filter = np.where((y_tst == 5) | (y_tst == 6))\n",
    "X_tr_b, y_tr_b = X_tr[train_filter], y_tr[train_filter]\n",
    "X_tst_b, y_tst_b = X_tst[test_filter], y_tst[test_filter]\n",
    "\n",
    "#Convert to 0 and 1\n",
    "y_tr_binary = (y_tr_b == 5).astype(np.float)\n",
    "y_tst_binary = (y_tst_b == 5).astype(np.float)\n",
    "\n",
    "y_tr_binary_expand = np.expand_dims(y_tr_binary, axis=1)\n",
    "y_tst_binary_expand = np.expand_dims(y_tst_binary, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return np.exp(t) / (1 + np.exp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    p = sigmoid(tx.dot(w))\n",
    "    loss = np.sum(-y * np.log(p) - (1 - y) * np.log(1 - p)) / y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    p = sigmoid(tx.dot(w))\n",
    "    gradient = np.dot(tx.T, (p - y)) / y.shape[0]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, tau):\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    w -= tau * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(tx, w):\n",
    "    return sigmoid(tx.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tx, w, threshold=0.5):\n",
    "    return predict_probs(tx, w) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x, max_iter, tau):\n",
    "    # init parameters\n",
    "    max_iter = max_iter\n",
    "    threshold = 1e-8\n",
    "    tau = tau #change to see the diff\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # Start GD\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, tau)\n",
    "        prediction = predict(tx, w, threshold=0.5).astype(np.float)\n",
    "#         print(prediction)\n",
    "        accuracy = (prediction == y).mean()\n",
    "        tprediction = predict(np.c_[np.ones((y_tst_b.shape[0], 1)), X_tst_b], w, threshold=0.5).astype(np.float)\n",
    "        taccuracy = (tprediction == y_tst_binary_expand).mean()\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}, training accuracy={a}, test accuracy={ta}\"\n",
    "                  .format(i=iter, l=loss, a=accuracy, ta=taccuracy))\n",
    "        # converge criterion\n",
    "        accuracies.append(accuracy)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    # Print result\n",
    "    print(\"Binary GD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    # visualization\n",
    "    #plt.plot(losses)\n",
    "    plt.plot(accuracies)\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "    # return for confusion matrix\n",
    "    y_pred = prediction.flatten()\n",
    "    y_act = y.flatten()\n",
    "    return y_pred, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_act = logistic_regression_gradient_descent_demo(y_tr_binary_expand, X_tr_b, 1000, 1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Confusion Matrix\n",
    "# np.set_printoptions(suppress=True) # disable scientific numbers\n",
    "y_act = pd.Series(y_act, name='Actual')\n",
    "y_pred = pd.Series(y_pred, name='Predicted')\n",
    "conf_mat = pd.crosstab(y_act, y_pred)\n",
    "# Normalized confusion matrix\n",
    "conf_mat_norm = conf_mat / conf_mat.sum(axis=1)\n",
    "# print(df_confusion)\n",
    "# sn.heatmap(conf_mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['5', '6']\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "# plt.title('Binary Confusion Matrix',fontsize=16)\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    ax.text(j, i, '{:0}'.format(z), ha='center', va='center') # for default in integer\n",
    "#     ax.text(j, i, '{:0.2%}'.format(z), ha='center', va='center') # for normalized in percentage\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    p = sigmoid(tx.dot(w))\n",
    "    loss = np.sum(-y * np.log(p) - (1 - y) * np.log(1 - p)) / y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tx, w, threshold=0.5):\n",
    "    return predict_probs(tx, w) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w, alpha):\n",
    "    p = sigmoid(tx.dot(w))\n",
    "    diag = np.diag((p * (1 - p)).flatten())\n",
    "    hessian = tx.T.dot(diag).dot(tx) / y.shape[0] + alpha * np.identity(w.shape[0])\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w, alpha):\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    hessian = calculate_hessian(y, tx, w, alpha)\n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(m):\n",
    "    a, b = m.shape\n",
    "    if a != b:\n",
    "        raise ValueError(\"Only square matrices are invertible.\")\n",
    "    i = np.eye(a,a)\n",
    "    return np.linalg.lstsq(m, i, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, alpha):\n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w, alpha)\n",
    "    loss += (alpha/2) * np.sum(w*w)\n",
    "    gradient += alpha*w\n",
    "    w -= np.linalg.solve(hessian, gradient)\n",
    "    #w -= np.dot(inv(hessian), gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    tau = 1e-7\n",
    "    alpha = 0.01\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # Start Newton's\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, alpha)\n",
    "        # log info\n",
    "        prediction = predict(tx, w, threshold=0.5).astype(np.float)\n",
    "        accuracy = (prediction == y).mean()\n",
    "        tprediction = predict(np.c_[np.ones((y_tst_b.shape[0], 1)), X_tst_b], w, threshold=0.5).astype(np.int)\n",
    "        taccuracy = (tprediction == y_tst_binary_expand).mean()\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}, training accuracy={a}, test accuracy={ta}\"\n",
    "                  .format(i=iter, l=loss, a=accuracy, ta=taccuracy))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    # Print result\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Newton's: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    # visualization\n",
    "    #plt.plot(losses)\n",
    "    plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_newton_method_demo(y_tr_binary_expand, X_tr_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, w, batch_size, max_iter, tau, alpha):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for n_iter in range(max_iter):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            gradient = calculate_gradient(y_batch, tx_batch, w)\n",
    "            gradient += alpha*w\n",
    "            # calculate loss\n",
    "            loss = calculate_loss(y, tx, w)\n",
    "            loss += (alpha/2) * np.sum(w*w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w -= tau * gradient\n",
    "            \n",
    "            prediction = predict(tx, w, threshold=0.5).astype(np.float)\n",
    "            accuracy = (prediction == y).mean()\n",
    "            tprediction = predict(np.c_[np.ones((y_tst_b.shape[0], 1)), X_tst_b], w, threshold=0.5).astype(np.float)\n",
    "            taccuracy = (tprediction == y_tst_binary_expand).mean()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "        # visualization\n",
    "        plt.plot(accuracies)\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, training accuracy={a}, test accuracy={ta}\".format(\n",
    "              bi=n_iter, ti=max_iter - 1, l=loss, a=accuracy, ta=taccuracy))\n",
    "    return loss, w, accuracy, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent_demo(y, x, max_iter, tau):\n",
    "    # init parameters\n",
    "    max_iter = max_iter\n",
    "    threshold = 1e-8\n",
    "    tau = tau #change to see the diff\n",
    "    alpha = 1e-7\n",
    "    batch_size = 1\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # Start SGD\n",
    "    start_time = datetime.datetime.now()\n",
    "    # start the logistic regression\n",
    "    # get loss and update w.\n",
    "    loss, w, accuracy, prediction = stochastic_gradient_descent(y, tx, w, batch_size, max_iter, tau, alpha)\n",
    "    #taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "    # log info\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    # Print result\n",
    "    print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "    \n",
    "    # return for confusion matrix\n",
    "    y_pred = prediction.flatten()\n",
    "    y_act = y.flatten()\n",
    "    return y_pred, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_act = logistic_regression_stochastic_gradient_descent_demo(y_tr_binary_expand, X_tr_b, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Confusion Matrix\n",
    "# np.set_printoptions(suppress=True) # disable scientific numbers\n",
    "y_act = pd.Series(y_act, name='Actual')\n",
    "y_pred = pd.Series(y_pred, name='Predicted')\n",
    "conf_mat = pd.crosstab(y_act, y_pred)\n",
    "# Normalized confusion matrix\n",
    "conf_mat_norm = conf_mat / conf_mat.sum(axis=1)\n",
    "# print(df_confusion)\n",
    "# sn.heatmap(conf_mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['5', '6']\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "# plt.title('Binary Confusion Matrix',fontsize=16)\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    ax.text(j, i, '{:0}'.format(z), ha='center', va='center') # for default in integer\n",
    "#     ax.text(j, i, '{:0.2%}'.format(z), ha='center', va='center') # for normalized in percentage\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(y):\n",
    "    return (np.arange(np.max(y) + 1) == y[:, None]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    softm = softmax(tx.dot(w))\n",
    "    loss = - np.sum(y * np.log(softm)) / y.shape[0]\n",
    "    prediction = np.argmax(softm, axis=1)\n",
    "    return softm, loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    softm = softmax(tx.dot(w))\n",
    "    return - np.dot(tx.T, (y - softm)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, tau, alpha):\n",
    "    y_enc = onehotencoding(y)\n",
    "    softm, loss, prediction = calculate_loss(y_enc, tx, w)\n",
    "    loss += (alpha/2) * np.sum(w*w)\n",
    "    gradient = calculate_gradient(y_enc, tx, w)\n",
    "    gradient += alpha\n",
    "    w -= tau * gradient\n",
    "    accuracy = np.sum(prediction == y)/ y.shape[0]\n",
    "    return loss, w, accuracy, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(x, y, w):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    softm = softmax(tx.dot(w))\n",
    "    prediction = np.argmax(softm, axis=1)\n",
    "    accuracy = np.sum(prediction == y)/ y.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x, max_iter, tau):\n",
    "    # init parameters\n",
    "    max_iter = max_iter\n",
    "    threshold = 1e-8\n",
    "    tau = tau #change to see the diff\n",
    "    alpha = 1e-6\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 10))\n",
    "    \n",
    "    # Start GD\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w, accuracy, prediction = learning_by_gradient_descent(y, tx, w, tau, alpha)\n",
    "        taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "        tprediction = np.argmax(softmax(np.c_[np.ones((y_tst.shape[0], 1)), X_tst].dot(w)), axis=1)\n",
    "        # log info\n",
    "        if iter % 50 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}, training accuracy={a}, test accuracy={ta}\"\n",
    "                  .format(i=iter, l=loss, a=accuracy, ta=taccuracy))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # Print result\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    # visualization\n",
    "    #plt.plot(losses)\n",
    "    plt.plot(accuracies)\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(onehotencoding(y), tx, w)))\n",
    "    # return for confusion matrix\n",
    "    y_pred = prediction.flatten()\n",
    "    y_act = y.flatten()\n",
    "    return y_pred, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred, y_act = logistic_regression_gradient_descent_demo(y_tr, X_tr, 1000, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Confusion Matrix\n",
    "# np.set_printoptions(suppress=True) # disable scientific numbers\n",
    "y_act = pd.Series(y_act, name='Actual')\n",
    "y_pred = pd.Series(y_pred, name='Predicted')\n",
    "conf_mat = pd.crosstab(y_act, y_pred, margins=True)\n",
    "# print(conf_mat)\n",
    "# Normalized confusion matrix\n",
    "conf_mat_norm = conf_mat / conf_mat.sum(axis=1)\n",
    "# print(conf_mat_norm)\n",
    "# print(df_confusion)\n",
    "# sn.heatmap(conf_mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "# plt.title('Binary Confusion Matrix',fontsize=16)\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "for (i, j), z in np.ndenumerate(conf_mat):\n",
    "    ax.text(j, i, '{:0}'.format(z), ha='center', va='center') # for default in integer\n",
    "#     ax.text(j, i, '{:0.2%}'.format(z), ha='center', va='center') # for normalized in percentage\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (Multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, w, batch_size, max_iter, tau, alpha):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    y_enc = onehotencoding(y)\n",
    "    for n_iter in range(max_iter):\n",
    "        for y_batch, tx_batch in batch_iter(y_enc, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            gradient = calculate_gradient(y_batch, tx_batch, w)\n",
    "            gradient += alpha*w\n",
    "            # calculate loss\n",
    "            softm, loss, prediction = calculate_loss(y_enc, tx, w)\n",
    "            loss += (alpha/2) * np.sum(w*w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w -= tau * gradient\n",
    "            accuracy = np.sum(prediction == y)/ y.shape[0]\n",
    "            taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, training accuracy={a}, test accuracy={ta}\".format(\n",
    "              bi=n_iter, ti=max_iter - 1, l=loss, a=accuracy, ta=taccuracy))\n",
    "        # visualization\n",
    "        plt.plot(accuracies)\n",
    "    return loss, w, accuracy, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent_demo(y, x, max_iter, tau):\n",
    "    # init parameters\n",
    "    max_iter = max_iter\n",
    "    threshold = 1e-8\n",
    "    tau = tau #change to see the diff\n",
    "    alpha = 1e-7\n",
    "    batch_size = 1\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 10))\n",
    "\n",
    "    # Start SGD\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # start the logistic regression\n",
    "    # get loss and update w.\n",
    "    loss, w, accuracy, prediction = stochastic_gradient_descent(y, tx, w, batch_size, max_iter, tau, alpha)\n",
    "    #taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "    # log info\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    # Print result\n",
    "    print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    \n",
    "    # return for confusion matrix\n",
    "    y_pred = prediction.flatten()\n",
    "    y_act = y.flatten()\n",
    "    return y_pred, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_act = logistic_regression_stochastic_gradient_descent_demo(y_tr, X_tr, 1000, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fashion MNIST dataset\n",
    "This dataset is taken from https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_images_tr, fashion_labels_tr = load_mnist('FashionMNIST/')\n",
    "fashion_images_tst, fashion_labels_tst = load_mnist('FashionMNIST/', 't10k')\n",
    "\n",
    "X_tr, y_tr = fashion_images_tr, fashion_labels_tr\n",
    "X_tst, y_tst = fashion_images_tst, fashion_labels_tst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X_tr[:12000:600], X_tr[13000:30600:600], X_tr[30600:60000:590]]\n",
    "plot_images(example_images, images_per_row=10)\n",
    "# save_fig(\"more_images_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images for each label\n",
    "X_0 = X_tr[(y_tr == 0)]\n",
    "X_1 = X_tr[(y_tr == 1)]\n",
    "X_2 = X_tr[(y_tr == 2)]\n",
    "X_3 = X_tr[(y_tr == 3)]\n",
    "X_4 = X_tr[(y_tr == 4)]\n",
    "X_5 = X_tr[(y_tr == 5)]\n",
    "X_6 = X_tr[(y_tr == 6)]\n",
    "X_7 = X_tr[(y_tr == 7)]\n",
    "X_8 = X_tr[(y_tr == 8)]\n",
    "X_9 = X_tr[(y_tr == 9)]\n",
    "\n",
    "plt.figure(figsize=(24,24))\n",
    "plt.subplot(521); plot_images(X_0[:25], images_per_row=5)\n",
    "plt.subplot(522); plot_images(X_1[:25], images_per_row=5)\n",
    "plt.subplot(523); plot_images(X_2[:25], images_per_row=5)\n",
    "plt.subplot(524); plot_images(X_3[:25], images_per_row=5)\n",
    "plt.subplot(525); plot_images(X_4[:25], images_per_row=5)\n",
    "plt.subplot(526); plot_images(X_5[:25], images_per_row=5)\n",
    "plt.subplot(527); plot_images(X_6[:25], images_per_row=5)\n",
    "plt.subplot(528); plot_images(X_7[:25], images_per_row=5)\n",
    "plt.subplot(529); plot_images(X_8[:25], images_per_row=5)\n",
    "plt.subplot(5,2,10); plot_images(X_9[:25], images_per_row=5)\n",
    "# save_fig(\"images_for_each_label\")\n",
    "plt.show()\n",
    "\n",
    "print(X_0.shape)\n",
    "print(X_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute your cost by negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    threshold = 1e-8\n",
    "    tau = 1e-6 #change to see the diff\n",
    "    alpha = 1e-7\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 10))\n",
    "\n",
    "    # Start GD\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w, accuracy = learning_by_gradient_descent(y, tx, w, tau, alpha)\n",
    "        taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "        # log info\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}, training accuracy={a}, test accuracy={ta}\"\n",
    "                  .format(i=iter, l=loss, a=accuracy, ta=taccuracy))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # Print result\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    # visualization\n",
    "    #plt.plot(losses)\n",
    "    plt.plot(accuracies)\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(onehotencoding(y), tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logistic_regression_penalized_gradient_descent_demo(y_tr, X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, w, batch_size, max_iter, tau, alpha):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    y_enc = onehotencoding(y)\n",
    "    for n_iter in range(max_iter):\n",
    "        for y_batch, tx_batch in batch_iter(y_enc, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            gradient = calculate_gradient(y_batch, tx_batch, w)\n",
    "            gradient += alpha*w\n",
    "            # calculate loss\n",
    "            softm, loss, prediction = calculate_loss(y_enc, tx, w)\n",
    "            loss += (alpha/2) * np.sum(w*w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w -= tau * gradient\n",
    "            accuracy = np.sum(prediction == y)/ y.shape[0]\n",
    "            taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "            losses.append(loss)\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, training accuracy={a}, test accuracy={ta}\".format(\n",
    "              bi=n_iter, ti=max_iter - 1, l=loss, a=accuracy, ta=taccuracy))\n",
    "    return loss, w, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    threshold = 1e-8\n",
    "    tau = 1e-6 #change to see the diff\n",
    "    alpha = 1e-7\n",
    "    batch_size = 1\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 10))\n",
    "\n",
    "    # Start SGD\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # start the logistic regression\n",
    "    # get loss and update w.\n",
    "    loss, w, accuracy = stochastic_gradient_descent(y, tx, w, batch_size, max_iter, tau, alpha)\n",
    "    #taccuracy = getAccuracy(X_tst, y_tst, w)\n",
    "    # log info\n",
    "    # Print result\n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    # visualization\n",
    "    plt.plot(accuracies)\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(onehotencoding(y), tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_stochastic_gradient_descent_demo(y_tr, X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax newton \n",
    "#grid search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
